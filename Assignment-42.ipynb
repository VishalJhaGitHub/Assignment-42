{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "006c1599-653b-4a1d-89cf-e4fd9da69ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values.\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Missing values in a dataset are simply values that are absent or unknown for certain observations or features. This could happen for a variety of reasons, such as errors in data collection, survey non-response, or data loss during transmission.\n",
    "\n",
    "#Handling missing values is essential because they can negatively impact the accuracy and reliability of data analysis and machine learning models. When missing values are present, they can cause biases in statistical analyses and lead to incorrect conclusions. Additionally, many machine learning algorithms cannot handle missing values, so it is important to address them before training any models.\n",
    "\n",
    "#Some algorithms that are not affected by missing values include decision trees, random forests, and gradient boosting. These algorithms are able to handle missing values by partitioning data based on available features or by imputing missing values with estimates such as the mean or median. Other algorithms that can handle missing values with some modifications include k-nearest neighbors, support vector machines, and neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4464746-1ef4-40ab-a54f-83cb4c812d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. List down techniques used to handle missing data. Give an example of each with python code.\n",
    "\n",
    "#Ans\n",
    "\n",
    "#1 - Deletion: Delete the rows or columns that contain missing values from the dataset. This method is simple and straightforward, but it can result in a loss of valuable data if too many rows or columns are deleted.\n",
    "\n",
    "#Example code:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# create sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4],\n",
    "                   'B': [5, np.nan, 7, 8],\n",
    "                   'C': [9, 10, 11, 12]})\n",
    "\n",
    "# drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# drop columns with missing values\n",
    "df = df.dropna(axis=1)\n",
    "\n",
    "#2 - Imputation: Fill in missing values with estimates such as the mean, median, mode, or predicted values from a regression model. This method preserves all rows and columns but may introduce bias if the imputed values are not accurate.\n",
    "\n",
    "#Example code:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# create sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4],\n",
    "                   'B': [5, np.nan, 7, 8],\n",
    "                   'C': [9, 10, 11, 12]})\n",
    "\n",
    "# impute missing values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "#3 - Regression imputation: Predict missing values using a regression model based on other features in the dataset. This method is more accurate than simple imputation but requires a strong correlation between the missing feature and other features in the dataset.\n",
    "\n",
    "#Example code:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4],\n",
    "                   'B': [5, np.nan, 7, 8],\n",
    "                   'C': [9, 10, 11, 12]})\n",
    "\n",
    "# impute missing values with regression model\n",
    "imputer = KNNImputer(n_neighbors=3, weights='uniform')\n",
    "X = df.drop('B', axis=1)\n",
    "y = df['B']\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_imputed, y)\n",
    "y_pred = regressor.predict(X_imputed)\n",
    "df_imputed = pd.DataFrame({'A': X_imputed['A'], 'B': y_pred, 'C': X_imputed['C']})\n",
    "\n",
    "#4 - Multiple imputation: Generate multiple imputed datasets by estimating missing values multiple times using different models and combining the results. This method produces more accurate results than single imputation and captures the uncertainty associated with missing values.\n",
    "\n",
    "#Example code:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# create sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4],\n",
    "                   'B': [5, np.nan, 7, 8],\n",
    "                   'C': [9, 10, 11, 12]})\n",
    "\n",
    "# generate multiple imputed datasets\n",
    "imputer = IterativeImputer(random_state=0)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dc9968c-f9c6-4f56-ba3b-95742a04b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Imbalanced data is a term used to describe a dataset where the number of observations in one class or category is much larger or much smaller than the number of observations in the other class or categories. In other words, the classes or categories are not evenly represented in the dataset.\n",
    "\n",
    "#For example, in a medical dataset, the number of healthy patients may be much larger than the number of patients with a particular disease, or in a credit card fraud detection dataset, the number of legitimate transactions may be much larger than the number of fraudulent transactions.\n",
    "\n",
    "#If imbalanced data is not handled, it can cause problems for predictive modeling and analysis. In particular, if the dataset is used to train a machine learning algorithm, the algorithm may learn to favor the majority class and ignore the minority class. This can result in a model that has high accuracy for the majority class but poor accuracy for the minority class. In other words, the model will be biased towards predicting the majority class, which can lead to false negatives or false positives for the minority class.\n",
    "\n",
    "#For example, in a medical dataset, a model that is biased towards predicting healthy patients may miss a significant number of patients who have the disease. Similarly, in a credit card fraud detection dataset, a model that is biased towards predicting legitimate transactions may miss a significant number of fraudulent transactions.\n",
    "\n",
    "#Therefore, it is essential to handle imbalanced data to ensure that machine learning models and analyses are accurate and reliable. This can be done by using techniques such as resampling, using different performance metrics, adjusting class weights, or using different algorithms that are designed to handle imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "223850d6-ea65-4097-96fd-094ece625b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. What are Up-sampling and Down-sampling? Explain with an example when up-sampling and downsampling are required.\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Up-sampling and down-sampling are two techniques used to address the issue of imbalanced data in machine learning.\n",
    "\n",
    "#Down-sampling involves reducing the number of observations in the majority class to balance the dataset. This is typically done by randomly removing observations from the majority class until the dataset is balanced. Down-sampling is used when the dataset has a large number of observations in the majority class and a small number of observations in the minority class.\n",
    "\n",
    "#For example, suppose you have a dataset of credit card transactions where 99% of the transactions are legitimate and only 1% of the transactions are fraudulent. In this case, down-sampling may be used to reduce the number of legitimate transactions in the dataset so that the number of fraudulent transactions is roughly equal to the number of legitimate transactions.\n",
    "\n",
    "#Up-sampling, on the other hand, involves increasing the number of observations in the minority class to balance the dataset. This is typically done by randomly duplicating observations from the minority class until the dataset is balanced. Up-sampling is used when the dataset has a small number of observations in the minority class and a large number of observations in the majority class.\n",
    "\n",
    "#For example, suppose you have a dataset of medical records where only 5% of the patients have a rare disease. In this case, up-sampling may be used to increase the number of observations of patients with the disease so that the dataset is balanced.\n",
    "\n",
    "#It is important to note that up-sampling and down-sampling are just two of many techniques used to address imbalanced data. The choice of technique depends on the characteristics of the dataset and the goals of the analysis. Other techniques include using different performance metrics, adjusting class weights, or using different algorithms that are designed to handle imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "256ba042-2ad5-4540-a550-b83349614f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. What is data Augmentation? Explain SMOTE.\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Data augmentation is a technique used in machine learning to increase the size of a dataset by generating new observations that are similar to the existing ones. The purpose of data augmentation is to improve the performance of machine learning models by providing more data to learn from.\n",
    "\n",
    "#SMOTE (Synthetic Minority Over-sampling Technique) is a specific data augmentation technique used to address the issue of imbalanced data in machine learning. SMOTE works by generating new observations of the minority class by creating synthetic examples that are similar to the existing minority class examples.\n",
    "\n",
    "#Here's how SMOTE works in practice:\n",
    "\n",
    "#1 - Identify the minority class: First, we need to identify the minority class in our dataset. This is the class that we want to generate new examples for.\n",
    "\n",
    "#2 - Identify the k-nearest neighbors: For each observation in the minority class, we identify its k-nearest neighbors in the dataset.\n",
    "\n",
    "#3 - Generate synthetic examples: For each observation in the minority class, we randomly select one of its k-nearest neighbors and use it to create a new synthetic observation. This new observation is generated by interpolating between the original observation and the selected neighbor, creating a new observation that is similar to the original but slightly different.\n",
    "\n",
    "#4 - Repeat the process: We repeat this process until we have generated enough new observations to balance the dataset.\n",
    "\n",
    "#By generating new observations in this way, SMOTE can help to balance the dataset and improve the performance of machine learning models. It is important to note that SMOTE should only be used on the training set and not on the test set to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d29a37a-03a6-4c39-814a-44ba0be378cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Outliers are data points that are significantly different from other observations in a dataset. Outliers can be caused by measurement or data entry errors, or they may be legitimate data points that represent extreme values in the population. Outliers can have a significant impact on the results of statistical analyses and machine learning models and can lead to inaccurate conclusions.\n",
    "\n",
    "#It is essential to handle outliers because they can have a disproportionate impact on the results of data analysis. Outliers can skew the mean and standard deviation of a dataset, leading to inaccurate estimates of central tendency and variability. In addition, outliers can affect the assumptions of statistical tests, leading to incorrect conclusions. In machine learning, outliers can affect the performance of models by causing them to overfit or underfit the data.\n",
    "\n",
    "#Handling outliers involves identifying them and deciding how to deal with them. One approach is to remove outliers from the dataset, either by deleting the observations or by replacing them with a more appropriate value. Another approach is to use statistical techniques that are robust to outliers, such as the median or trimmed mean. In machine learning, outliers can be handled by using algorithms that are less sensitive to outliers, such as decision trees or support vector machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c43a5827-446d-4a47-b9a6-50b36cca1df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#There are several techniques that can be used to handle missing data in customer data analysis. Here are a few:\n",
    "\n",
    "#1 - Delete missing data: If the amount of missing data is small, one option is to simply remove the missing observations from the dataset. However, this approach can lead to a loss of information and reduced sample size.\n",
    "\n",
    "#2 - Impute missing data: Imputation is a technique that involves estimating missing values based on the available data. This can be done using statistical techniques such as mean imputation, regression imputation, or multiple imputation.\n",
    "\n",
    "#3 - Create a separate category for missing data: For categorical variables, it may be appropriate to create a separate category for missing data. This can be done by assigning a unique code or label to missing data.\n",
    "\n",
    "#4 - Use domain knowledge: In some cases, domain knowledge can be used to impute missing data. For example, if a customer's age is missing, it may be possible to estimate their age based on their date of birth or other demographic information.\n",
    "\n",
    "#5 - Use machine learning: Machine learning algorithms such as decision trees and random forests can handle missing data by automatically choosing the best imputation method based on the available data.\n",
    "\n",
    "#The choice of technique will depend on the amount and pattern of missing data, the type of analysis being performed, and the specific characteristics of the dataset. It is important to carefully consider the implications of each technique and choose the one that is most appropriate for the analysis at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e190c0f6-6266-4d03-916a-6c7b45eea3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#There are several strategies that can be used to determine if missing data is missing at random or if there is a pattern to the missing data. Here are a few:\n",
    "\n",
    "#1 - Missing data analysis: One approach is to conduct a missing data analysis, which involves examining the patterns of missing data and determining if there are any relationships between missing data and other variables in the dataset. This can be done using statistical techniques such as correlation analysis or regression analysis.\n",
    "\n",
    "#2 - Imputation: Imputation can also be used to determine if the missing data is missing at random. If the imputed values are consistent with the observed data, it is likely that the missing data is missing at random. If the imputed values are inconsistent with the observed data, it is likely that there is a pattern to the missing data.\n",
    "\n",
    "#3 - Subgroup analysis: Another approach is to conduct subgroup analyses on the variables with missing data to see if there are any patterns in the missing data. For example, if the missing data is only present in a specific demographic group, it is likely that there is a pattern to the missing data.\n",
    "\n",
    "#4 - Visual inspection: Visual inspection can be a useful tool for identifying patterns in missing data. This can be done by creating visualizations such as scatterplots or histograms to examine the distribution of missing data across the dataset.\n",
    "\n",
    "#By using these strategies, it is possible to determine if the missing data is missing at random or if there is a pattern to the missing data. This information can be used to guide the selection of appropriate data handling techniques and to ensure that the results of the analysis are accurate and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "511265e6-c563-4675-982e-ae89150c7525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Imbalanced datasets are common in medical diagnosis projects, where the prevalence of a condition can be low. Here are some strategies to evaluate the performance of a machine learning model on an imbalanced dataset:\n",
    "\n",
    "#1 - Use evaluation metrics that are robust to class imbalance: Accuracy is not a reliable metric to evaluate the performance of a model on an imbalanced dataset, as it can be skewed by the large number of negative cases. Instead, metrics such as precision, recall, F1 score, and area under the receiver operating characteristic (ROC) curve are more appropriate for imbalanced datasets.\n",
    "\n",
    "#2 - Use resampling techniques: Resampling techniques can be used to balance the classes in the dataset. Two commonly used techniques are oversampling and undersampling. In oversampling, the minority class is oversampled by replicating its instances. In undersampling, the majority class is undersampled by randomly removing instances. Both techniques have their own advantages and disadvantages and should be used with caution.\n",
    "\n",
    "#3 - Use cost-sensitive learning: Cost-sensitive learning is a technique that assigns different misclassification costs to different classes. This approach can be used to penalize misclassification of the minority class more heavily than the majority class.\n",
    "\n",
    "#4 - Use ensemble methods: Ensemble methods, such as bagging, boosting, and stacking, can be used to combine multiple models and improve the overall performance on imbalanced datasets.\n",
    "\n",
    "#5 - Use domain knowledge: Domain knowledge can be used to guide the selection of evaluation metrics and resampling techniques. For example, in medical diagnosis projects, false negatives may be more costly than false positives, and therefore, recall may be a more important metric to optimize.\n",
    "\n",
    "#By using these strategies, it is possible to evaluate the performance of a machine learning model on an imbalanced dataset and improve the accuracy of the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15bc62fb-d3a2-4387-b83d-a944475d6283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#To balance an unbalanced dataset, where the majority of customers are reporting satisfaction, down-sampling the majority class can be used. Here are some methods that can be employed to down-sample the majority class:\n",
    "\n",
    "#1 - Random under-sampling: This involves randomly selecting a subset of instances from the majority class to match the number of instances in the minority class. This method is simple to implement but can result in loss of information.\n",
    "\n",
    "#2 - Cluster-based under-sampling: This method involves clustering instances from the majority class and selecting instances from each cluster to match the number of instances in the minority class. This method can be effective in retaining information and reducing the impact of outliers.\n",
    "\n",
    "#3 - Tomek links: This method involves identifying pairs of instances from the majority and minority class that are close to each other and removing the instance from the majority class. This method can be effective in removing noisy instances but can result in loss of information.\n",
    "\n",
    "#4 - NearMiss algorithm: This method selects instances from the majority class that are closest to the instances in the minority class. This method can be effective in retaining information and preserving the distribution of the data.\n",
    "\n",
    "#To down-sample the majority class, one or more of these methods can be used to select a subset of instances from the majority class that matches the number of instances in the minority class. By doing so, the dataset can be balanced, and machine learning models can be trained to better predict customer satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "988e3b68-ca8b-457a-afc5-b0e4b554cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#To balance an unbalanced dataset with a low percentage of occurrences, where the minority class is the rare event of interest, up-sampling the minority class can be used. Here are some methods that can be employed to up-sample the minority class:\n",
    "\n",
    "#1 - Random over-sampling: This involves randomly replicating instances from the minority class to match the number of instances in the majority class. This method is simple to implement but can result in overfitting and bias towards the minority class.\n",
    "\n",
    "#2 - Synthetic Minority Over-sampling Technique (SMOTE): This method involves creating synthetic instances from the minority class by interpolating between pairs of similar instances. This method can be effective in retaining information and avoiding overfitting.\n",
    "\n",
    "#3 - Adaptive Synthetic (ADASYN): This method is an extension of SMOTE that generates more synthetic samples for the minority class samples that are harder to learn.\n",
    "\n",
    "#4 - Random Minority Over-sampling Technique (RMOT): This method is a variant of random over-sampling that randomly selects a subset of the minority class and replicates it until the desired balance is achieved.\n",
    "\n",
    "#To up-sample the minority class, one or more of these methods can be used to create synthetic or replicate instances from the minority class to match the number of instances in the majority class. By doing so, the dataset can be balanced, and machine learning models can be trained to better predict the occurrence of the rare event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd49a6d8-8719-4894-8c67-f60ed2375b96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
